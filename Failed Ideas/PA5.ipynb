{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5\n",
    "\n",
    "## Language models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Joshua Burris*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we will explore language models.\n",
    "To do so, we will encapsulate a language model using a class called `language_model` which implements language models with Laplace add-one smoothing.\n",
    "An overview of the methods your class should have is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "class language_model:\n",
    "    def __init__(self, ngram=1) :\n",
    "        \"\"\"\n",
    "        Initialize a language model\n",
    "\n",
    "        Parameters:\n",
    "        ngram specifies the type of model:  \n",
    "        unigram (ngram = 1), bigram (ngram = 2) etc.\n",
    "        \"\"\"\n",
    "        self.ngram = ngram\n",
    "        \n",
    "\n",
    "    def train(self, file_name) :\n",
    "        \"\"\"\n",
    "        train a language model\n",
    "\n",
    "        Parameters:\n",
    "        file_name is a file that contains the training set for the model\n",
    "        \"\"\"\n",
    "        \n",
    "        self.text = self.getText(file_name)\n",
    "        if self.ngram > 1 :\n",
    "            self.bigram_data = []\n",
    "            for i in range(len(self.text) - 1) :\n",
    "                self.bigram_data.append(self.text[i] +' '+ self.text[i+1])\n",
    "            self.bigram_data = Counter(self.bigram_data)\n",
    "        if self.ngram > 2 :\n",
    "            self.trigram_data = []\n",
    "            for i in range(len(self.text) - self.ngram + 1) :\n",
    "                gram = self.text[i]\n",
    "                for j in range(1, self.ngram) :\n",
    "                    gram += ' '+ self.text[i+j]\n",
    "                self.trigram_data.append(gram)\n",
    "            self.trigram_data = Counter(self.trigram_data)\n",
    "        \n",
    "        self.frequency_data = Counter(self.text)\n",
    "        \n",
    "        self.V = len(self.frequency_data)\n",
    "        self.total_count = sum(self.frequency_data.values())\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def test(self, file_name) :\n",
    "        \"\"\"\n",
    "        Test a language model on a given text and return the perplexity \n",
    "        of a trained model on the text provided as input\n",
    "\n",
    "        Parameters:\n",
    "        file_name is a file that contains the test set on which the \n",
    "        model needs to be evaluated \n",
    "        \"\"\"\n",
    "        \n",
    "        text = self.getText(file_name)\n",
    "        \n",
    "        # Calculate sparsity\n",
    "        zero_entries = 0\n",
    "        entries = 0\n",
    "        for i in range(len(text) - self.ngram + 1) :\n",
    "            gram = text[i]\n",
    "            for j in range(1, self.ngram) :\n",
    "                gram += ' '+ text[i+j]\n",
    "            data = {}\n",
    "            if self.ngram == 1 :\n",
    "                data = self.frequency_data\n",
    "            elif self.ngram == 2 :\n",
    "                data = self.bigram_data\n",
    "            elif self.ngram == 3 :\n",
    "                data = self.trigram_data\n",
    "            if data.setdefault(gram, 0) == 0 :\n",
    "                zero_entries += 1\n",
    "            entries += 1\n",
    "        self.sparsity = zero_entries / entries\n",
    "        \n",
    "        return self.perplexity(text)\n",
    "    \n",
    "    def perplexity(self, text) :\n",
    "        return math.pow(2, self.entropy(text))\n",
    "    \n",
    "    def entropy(self, text) :\n",
    "        e = 0\n",
    "        for i in range(self.ngram - 1, len(text)) :\n",
    "            context = text[i - self.ngram + 1 : i] # This is the previous word/words for the bigram/trigram.\n",
    "            e += -math.log(self.probability(text[i], context), 2)     \n",
    "        return e / (len(text) - (self.ngram - 1))\n",
    "        \n",
    "    '''Word is a single word. Context is a list of words.'''\n",
    "    def probability(self, word, context) :\n",
    "        if self.ngram == 1 :\n",
    "            return (self.count([word]) + 1) / (self.total_count + self.V)\n",
    "        else :\n",
    "            return (self.count(context + [word]) + 1) / (self.count(context) + self.V)\n",
    "    \n",
    "    def count(self, context) :\n",
    "        length = len(context)\n",
    "        context = ' '.join(context)\n",
    "        if length == 1 :\n",
    "            return self.frequency_data.setdefault(context, 0)\n",
    "        elif length == 2 :\n",
    "            return self.bigram_data.setdefault(context, 0)\n",
    "        elif length == 3 :\n",
    "            return self.trigram_data.setdefault(context, 0)\n",
    "        return 0\n",
    "    \n",
    "    def getText(self, filename) :\n",
    "        \n",
    "        text = open(filename, 'r').read()\n",
    "        \n",
    "        # Convert the text to lower case.\n",
    "        text = text.lower()\n",
    "\n",
    "        # Convert question marks (?), colons (:) and exclamation marks (!) to periods.\n",
    "        # Dashes should be converted to spaces.\n",
    "        text = text.translate({63: 46, 58: 46, 33: 46, 45: 32})\n",
    "\n",
    "        # Remove all punctuation marks other than the period (commas, semicolons,\n",
    "        # underscores and quotes).\n",
    "        trans = str.maketrans('', '', string.punctuation.replace('.', ''))\n",
    "        text = text.translate(trans)\n",
    "\n",
    "        # Also replace whitespace with a single space.\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        # Parse the text into sentences, adding beginning of sentence and end of sentence tokens.\n",
    "        sentences = text.split('.')\n",
    "        text = []\n",
    "        for i in range(len(sentences)) :\n",
    "            sentences[i] = sentences[i].strip()\n",
    "            if sentences[i] is not '' :\n",
    "                text += ['<s>'] + sentences[i].split() + ['</s>']\n",
    "        \n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your tasks:\n",
    "\n",
    "Complete the above class for training and evaluating\n",
    "language models using unigrams, bigrams, and trigrams.\n",
    "Before using a text for training or testing:\n",
    "* Convert the text to lower case.\n",
    "* Convert question marks (?), colons (:) and exclamation marks (!) to periods.\n",
    "* Remove all punctuation marks other than the period (commas, semicolons, underscores and quotes); dashes should be converted to spaces.\n",
    "* Parse the text into sentences, adding beginning of sentence and end of sentence tokens.\n",
    "\n",
    "For performing the transformations of the text use Python's string method `str.translate`, which also uses a conversion table that is created by the string method `str.maketrans`.\n",
    "Finally for storing your language model, use a Python dictionary.  You may find it useful to use Python's `defaultdict`, which is part of the `collections` module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using the code you have written, demonstrate how you would use language models to distinguish between the writings of Jane Austen and some of her contemporaries.  We have provided the text for the following books for you to use in ASCII format:\n",
    "* Pride and Prejudice by Jane Austen\n",
    "* Persuasion by Jane Austen\n",
    "* Sense and Sensibility by Jane Austen\n",
    "* Jane Eyre by Charlotte Bronte\n",
    "* Alice in Wonderland by Lewis Carroll\n",
    "\n",
    "These can be downloaded [here](https://www.cs.colostate.edu/~cs440/fall19/assignments/texts.tar.gz).\n",
    "Original versions of the text are from the [Project Gutenberg](http://www.gutenberg.org/) website. \n",
    "\n",
    "In your analysis, compare the ability of unigram, bigram and trigram models for this task.  Explain your observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, analyze the models you have constructed for Pride and Prejudice.  How sparse are the bigram and trigram models?  I.e., what fraction of the total number of entries in the model before smoothing are zero?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gram: 1\n",
      "Train: pride_and_prejudice.txt Test: pride_and_prejudice.txt Perplexity: 445.5110872106086\n",
      "Sparsity: 0.0\n",
      "Train: pride_and_prejudice.txt Test: persuasion.txt Perplexity: 590.0455193284915\n",
      "Sparsity: 0.05362720706699839\n",
      "Train: pride_and_prejudice.txt Test: sense_and_sensibility.txt Perplexity: 571.2597150265282\n",
      "Sparsity: 0.05007125672341665\n",
      "Train: pride_and_prejudice.txt Test: jane_eyre.txt Perplexity: 694.1180654023972\n",
      "Sparsity: 0.10168234476802043\n",
      "Train: pride_and_prejudice.txt Test: alice_in_wonderland.txt Perplexity: 694.5026331277033\n",
      "Sparsity: 0.10397111913357401\n",
      "Train: persuasion.txt Test: pride_and_prejudice.txt Perplexity: 552.7067598222569\n",
      "Train: persuasion.txt Test: persuasion.txt Perplexity: 456.92171164182287\n",
      "Train: persuasion.txt Test: sense_and_sensibility.txt Perplexity: 580.5883834898078\n",
      "Train: persuasion.txt Test: jane_eyre.txt Perplexity: 685.6877970666901\n",
      "Train: persuasion.txt Test: alice_in_wonderland.txt Perplexity: 656.8630946446768\n",
      "Train: sense_and_sensibility.txt Test: pride_and_prejudice.txt Perplexity: 555.2121227900874\n",
      "Train: sense_and_sensibility.txt Test: persuasion.txt Perplexity: 600.027558146888\n",
      "Train: sense_and_sensibility.txt Test: sense_and_sensibility.txt Perplexity: 459.35644493947206\n",
      "Train: sense_and_sensibility.txt Test: jane_eyre.txt Perplexity: 697.3953633792607\n",
      "Train: sense_and_sensibility.txt Test: alice_in_wonderland.txt Perplexity: 689.633882489554\n",
      "Train: jane_eyre.txt Test: pride_and_prejudice.txt Perplexity: 682.1732084549296\n",
      "Train: jane_eyre.txt Test: persuasion.txt Perplexity: 731.2137038615587\n",
      "Train: jane_eyre.txt Test: sense_and_sensibility.txt Perplexity: 713.7880894953051\n",
      "Train: jane_eyre.txt Test: jane_eyre.txt Perplexity: 545.0567344136665\n",
      "Train: jane_eyre.txt Test: alice_in_wonderland.txt Perplexity: 624.5496085862811\n",
      "Train: alice_in_wonderland.txt Test: pride_and_prejudice.txt Perplexity: 579.5943002600104\n",
      "Train: alice_in_wonderland.txt Test: persuasion.txt Perplexity: 586.24620129862\n",
      "Train: alice_in_wonderland.txt Test: sense_and_sensibility.txt Perplexity: 592.6743617183644\n",
      "Train: alice_in_wonderland.txt Test: jane_eyre.txt Perplexity: 524.5781000373318\n",
      "Train: alice_in_wonderland.txt Test: alice_in_wonderland.txt Perplexity: 279.20192163151916\n",
      "\n",
      "Gram: 2\n",
      "Train: pride_and_prejudice.txt Test: pride_and_prejudice.txt Perplexity: 654.8804587921724\n",
      "Sparsity: 0.0\n",
      "Train: pride_and_prejudice.txt Test: persuasion.txt Perplexity: 1121.780685144128\n",
      "Sparsity: 0.37558088557650154\n",
      "Train: pride_and_prejudice.txt Test: sense_and_sensibility.txt Perplexity: 1100.2919995201592\n",
      "Sparsity: 0.3540796702244221\n",
      "Train: pride_and_prejudice.txt Test: jane_eyre.txt Perplexity: 1250.539781453379\n",
      "Sparsity: 0.45930950313328794\n",
      "Train: pride_and_prejudice.txt Test: alice_in_wonderland.txt Perplexity: 1268.7876901600873\n",
      "Sparsity: 0.449318530553299\n",
      "Train: persuasion.txt Test: pride_and_prejudice.txt Perplexity: 1173.2698855365697\n",
      "Train: persuasion.txt Test: persuasion.txt Perplexity: 734.833460824772\n",
      "Train: persuasion.txt Test: sense_and_sensibility.txt Perplexity: 1265.0279275307316\n",
      "Train: persuasion.txt Test: jane_eyre.txt Perplexity: 1356.133430773603\n",
      "Train: persuasion.txt Test: alice_in_wonderland.txt Perplexity: 1309.083416791062\n",
      "Train: sense_and_sensibility.txt Test: pride_and_prejudice.txt Perplexity: 1052.8199646943985\n",
      "Train: sense_and_sensibility.txt Test: persuasion.txt Perplexity: 1149.8290316083446\n",
      "Train: sense_and_sensibility.txt Test: sense_and_sensibility.txt Perplexity: 708.4200242443228\n",
      "Train: sense_and_sensibility.txt Test: jane_eyre.txt Perplexity: 1267.8615741312894\n",
      "Train: sense_and_sensibility.txt Test: alice_in_wonderland.txt Perplexity: 1264.6172868066492\n",
      "Train: jane_eyre.txt Test: pride_and_prejudice.txt Perplexity: 1944.0017073577135\n",
      "Train: jane_eyre.txt Test: persuasion.txt Perplexity: 2079.064900266614\n",
      "Train: jane_eyre.txt Test: sense_and_sensibility.txt Perplexity: 2088.8011012305515\n",
      "Train: jane_eyre.txt Test: jane_eyre.txt Perplexity: 1105.6223807789784\n",
      "Train: jane_eyre.txt Test: alice_in_wonderland.txt Perplexity: 1762.9384068332981\n",
      "Train: alice_in_wonderland.txt Test: pride_and_prejudice.txt Perplexity: 803.0732949194437\n",
      "Train: alice_in_wonderland.txt Test: persuasion.txt Perplexity: 808.672173439493\n",
      "Train: alice_in_wonderland.txt Test: sense_and_sensibility.txt Perplexity: 832.0121663597064\n",
      "Train: alice_in_wonderland.txt Test: jane_eyre.txt Perplexity: 736.7740507345765\n",
      "Train: alice_in_wonderland.txt Test: alice_in_wonderland.txt Perplexity: 320.7581519439975\n",
      "\n",
      "Gram: 3\n",
      "Train: pride_and_prejudice.txt Test: pride_and_prejudice.txt Perplexity: 2022.487280502607\n",
      "Sparsity: 0.0\n",
      "Train: pride_and_prejudice.txt Test: persuasion.txt Perplexity: 3899.715117305238\n",
      "Sparsity: 0.7418757329650698\n",
      "Train: pride_and_prejudice.txt Test: sense_and_sensibility.txt Perplexity: 3906.611146920063\n",
      "Sparsity: 0.7304309182297414\n",
      "Train: pride_and_prejudice.txt Test: jane_eyre.txt Perplexity: 4008.9759739075985\n",
      "Sparsity: 0.7858933315808744\n",
      "Train: pride_and_prejudice.txt Test: alice_in_wonderland.txt Perplexity: 4230.912679240346\n",
      "Sparsity: 0.7848889691280014\n",
      "Train: persuasion.txt Test: pride_and_prejudice.txt Perplexity: 3780.310477035127\n",
      "Train: persuasion.txt Test: persuasion.txt Perplexity: 2007.2615078643216\n",
      "Train: persuasion.txt Test: sense_and_sensibility.txt Perplexity: 3955.7644863718588\n",
      "Train: persuasion.txt Test: jane_eyre.txt Perplexity: 3946.679512611922\n",
      "Train: persuasion.txt Test: alice_in_wonderland.txt Perplexity: 4079.5807423575234\n",
      "Train: sense_and_sensibility.txt Test: pride_and_prejudice.txt Perplexity: 3814.0580737119726\n",
      "Train: sense_and_sensibility.txt Test: persuasion.txt Perplexity: 3984.9565061767244\n",
      "Train: sense_and_sensibility.txt Test: sense_and_sensibility.txt Perplexity: 2120.2855335247227\n",
      "Train: sense_and_sensibility.txt Test: jane_eyre.txt Perplexity: 4077.6470084675866\n",
      "Train: sense_and_sensibility.txt Test: alice_in_wonderland.txt Perplexity: 4251.449807358444\n",
      "Train: jane_eyre.txt Test: pride_and_prejudice.txt Perplexity: 7470.552591097011\n",
      "Train: jane_eyre.txt Test: persuasion.txt Perplexity: 7737.661871892484\n",
      "Train: jane_eyre.txt Test: sense_and_sensibility.txt Perplexity: 7836.255806547038\n",
      "Train: jane_eyre.txt Test: jane_eyre.txt Perplexity: 3653.840838312217\n",
      "Train: jane_eyre.txt Test: alice_in_wonderland.txt Perplexity: 7297.4504036601065\n",
      "Train: alice_in_wonderland.txt Test: pride_and_prejudice.txt Perplexity: 1350.0590051073846\n",
      "Train: alice_in_wonderland.txt Test: persuasion.txt Perplexity: 1352.0598463490744\n",
      "Train: alice_in_wonderland.txt Test: sense_and_sensibility.txt Perplexity: 1364.7247769553646\n",
      "Train: alice_in_wonderland.txt Test: jane_eyre.txt Perplexity: 1311.6434019712904\n",
      "Train: alice_in_wonderland.txt Test: alice_in_wonderland.txt Perplexity: 603.6220868654889\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_results() :\n",
    "    book_list = ['pride_and_prejudice.txt', 'persuasion.txt', 'sense_and_sensibility.txt', 'jane_eyre.txt', 'alice_in_wonderland.txt']\n",
    "    for i in range(3) :\n",
    "        print('Gram:', i+1)\n",
    "        model = language_model(i+1)\n",
    "        for train_book in book_list :\n",
    "            model.train(train_book)\n",
    "            for test_book in book_list :\n",
    "                print('Train:', train_book, 'Test:', test_book, 'Perplexity:', model.test(test_book))\n",
    "                if train_book == 'pride_and_prejudice.txt' :\n",
    "                    print('Sparsity:', model.sparsity)\n",
    "        print()\n",
    "    pass\n",
    "\n",
    "def test_data() :\n",
    "    gram = 1\n",
    "    model = language_model(gram)\n",
    "    model.train('alice_in_wonderland.txt')\n",
    "    print('Perplexity:', model.test('alice_in_wonderland.txt'))\n",
    "    print('Vocabulary size:', model.V)\n",
    "    print('Frequency data (top 10):')\n",
    "    for k, v in model.frequency_data.most_common(10) :\n",
    "        print('\\t', k + ':', v)\n",
    "    if gram > 1 :\n",
    "        print('Bigram data (top 10):')\n",
    "        for k, v in model.bigram_data.most_common(10) :\n",
    "            print('\\t', k + ':', v)\n",
    "    if gram > 2 :\n",
    "        print('Trigram data (top 10):')\n",
    "        for k, v in model.trigram_data.most_common(10) :\n",
    "            print('\\t', k + ':', v)\n",
    "    pass\n",
    "\n",
    "get_results()\n",
    "#test_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:\n",
    "\n",
    "### Unigram:\n",
    "| Training set ▼ \\| Test set ► | Pride and Prejudice | Persuasion | Sense and Sensibility | Jane Eyre | Alice in Wonderland |\n",
    "|-|-|-|-|-|-|\n",
    "| Pride and Prejudice | 446 | 590 | 571 | 694 | 695 |\n",
    "| Persuasion | 552 | 457 | 581 | 686 | 657 |\n",
    "| Sense and Sensibility | 555 | 600 | 459 | 697 | 690 |\n",
    "| Jane Eyre | 682 | 731 | 714 | 545 | 625 |\n",
    "| Alice in Wonderland | 580 | 586 | 593 | 525 | 279 |\n",
    "\n",
    "### Bigram:\n",
    "| Training set ▼ \\| Test set ► | Pride and Prejudice | Persuasion | Sense and Sensibility | Jane Eyre | Alice in Wonderland |\n",
    "|-|-|-|-|-|-|\n",
    "| Pride and Prejudice | 655 | 1122 | 1100 | 1251 | 1269 |\n",
    "| Persuasion | 1173 | 735 | 1265 | 1356 | 1309 |\n",
    "| Sense and Sensibility | 1053 | 1150 | 708 | 1268 | 1265 |\n",
    "| Jane Eyre | 1944 | 2079 | 2089 | 1106 | 1763 |\n",
    "| Alice in Wonderland | 803 | 809 | 832 | 737 | 320 |\n",
    "\n",
    "### Trigram:\n",
    "| Training set ▼ \\| Test set ► | Pride and Prejudice | Persuasion | Sense and Sensibility | Jane Eyre | Alice in Wonderland |\n",
    "|-|-|-|-|-|-|\n",
    "| Pride and Prejudice | 2022 | 3900 | 3907 | 4009 | 4231 |\n",
    "| Persuasion | 3780 | 2007 | 3956 | 3947 | 4080 |\n",
    "| Sense and Sensibility | 3814 | 3985 | 2120 | 4078 | 4251 |\n",
    "| Jane Eyre | 7471 | 7738 | 7836 | 3654 | 7297 |\n",
    "| Alice in Wonderland | 1350 | 1352 | 1365 | 1312 | 604 |\n",
    "\n",
    "### Pride and Prejudice sparsity comparison:\n",
    "| Ngram ▼ \\| Test set ► | Pride and Prejudice | Persuasion | Sense and Sensibility | Jane Eyre | Alice in Wonderland |\n",
    "|-|-|-|-|-|-|\n",
    "| Unigram | 0.000% | 5.363% | 5.007% | 10.168% | 10.397% |\n",
    "| Bigram | 0.000% | 37.558% | 35.408% | 45.931% | 44.932% |\n",
    "| Trigram | 0.000% | 74.188% | 73.043% | 78.589% | 78.489% |\n",
    "\n",
    "## Discussion:\n",
    "An ngram language model is used to determine how similar two pieces of text are to each other. It's clear that unigrams have the lower perplexity/better model over bigrams and trigrams. The perplexity for unigrams was lower in every scenario. Notably, when comparing the same texts to each other they almost always had the lowest perplexity compared to other novels. I would also say based on the data, Jane Austen's novels were more like her own than her contemporaries. There was a clear lower perplexity when comparing Jane Austen's books to themselves, and a slightly higher perplexity otherwise. \n",
    "\n",
    "When comparing the sparsity of Pride and Prejudice to other novels using bigrams and trigrams, bigrams had notably less sparsity. Sparsity is the fraction of the number of entries in the training model over the total number of entries in the testing model. This is expected because it reinforces our conclusion of comparing perplexity. A lower sparsity yields a lower perplexity, meaning the texts are more similar. I would say the bigram models weren't too sparse as the percentage was only around ~40%, but the trigram models were very sparse at around ~75%. The sparsity was also notably higher when comparing Jane Austen to her contemporaries, ~5% higher in each case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Answer the questions in the cells reserved for that purpose.\n",
    "Submit your report as a Jupyter notebook via Canvas. Running the notebook should generate all the results in your notebook.  Leave the output of your notebook intact so we don't have to run it to see the results.\n",
    "\n",
    "## Grading\n",
    "\n",
    "Grading will be based on the following criteria:\n",
    "\n",
    "* Your code solves the specified problem.\n",
    "* You performed an analysis that addresses the problem.\n",
    "* Overall readability and organization of the notebook.\n",
    "* Effort in making interesting observations where required.\n",
    "* Conciseness. Points may be taken off if the notebook is overly long."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
