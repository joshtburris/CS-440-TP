{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Clickbait Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team members: Joshua Burris, Caleb Tong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import string\n",
    "from collections import Counter\n",
    "class language_model:\n",
    "    def __init__(self, ngram=1) :\n",
    "        \"\"\"\n",
    "        Initialize a language model\n",
    "        \n",
    "        Parameters:\n",
    "        ngram specifies the type of model:  \n",
    "        unigram (ngram = 1), bigram (ngram = 2) etc.\n",
    "        \"\"\"\n",
    "        self.ngram = ngram\n",
    "        \n",
    "    def train(self, file_name) :\n",
    "        self.story = self.clean_text(file_name)\n",
    "        if self.ngram > 1:\n",
    "            self.bigram = []\n",
    "            for i in range(len(self.story) - 1):\n",
    "                self.bigram.append(self.story[i] + ' ' + self.story[i+1])\n",
    "            self.bigram = Counter(self.bigram)\n",
    "        if self.ngram > 2:\n",
    "            self.trigram = []\n",
    "            for i in range(len(self.story) - self.ngram + 1):\n",
    "                temp = self.story[i]\n",
    "                for j in range(1, self.ngram):\n",
    "                    temp += ' ' + self.story[i+j]\n",
    "                self.trigram.append(temp)\n",
    "            self.trigram = Counter(self.trigram)\n",
    "        self.data_frequency = Counter(self.story)\n",
    "        self.V = len(self.data_frequency)\n",
    "        self.total_count = sum(self.data_frequency.values())\n",
    "        #print(self.total_count, self.V, self.data_frequency)\n",
    "    \n",
    "    def test(self, file_name) :\n",
    "        text = self.clean_text(file_name)\n",
    "        \n",
    "        non_entries, entries = 0, 0\n",
    "        for i in range(len(text) - self.ngram + 1):\n",
    "            temp = text[i]\n",
    "            for j in range(1, self.ngram):\n",
    "                temp += ' ' + text[i+j]\n",
    "            data = {}\n",
    "            if self.ngram == 1:\n",
    "                data = self.data_frequency\n",
    "            elif self.ngram == 2:\n",
    "                data = self.bigram\n",
    "            elif self.ngram == 3:\n",
    "                data = self.trigram\n",
    "            if data.setdefault(temp, 0) == 0:\n",
    "                non_entries += 1\n",
    "            entries += 1\n",
    "        \n",
    "        self.sparsity = non_entries / entries\n",
    "        \n",
    "        return self.perplexity(text)\n",
    "    \n",
    "    def probability(self, word1, words):\n",
    "        if self.ngram == 1:\n",
    "            return (self.C([word1]) + 1) / (self.total_count + self.V)\n",
    "        else:\n",
    "            return (self.C(words + [word1]) + 1) / (self.C(words) + self.V)\n",
    "    \n",
    "    def perplexity(self, text):\n",
    "        return math.pow(2, self.entropy(text))\n",
    "    \n",
    "    def entropy(self, text):\n",
    "        exp = 0\n",
    "        for i in range(self.ngram - 1, len(text)) :\n",
    "            prevW = text[i - self.ngram + 1 : i]\n",
    "            exp += -math.log(self.probability(text[i], prevW), 2)     \n",
    "        return exp / (len(text) - (self.ngram - 1))\n",
    "    \n",
    "    def C(self, words):\n",
    "        size = len(words)\n",
    "        words = ' '.join(words)\n",
    "        if size == 1: return self.data_frequency.setdefault(words, 0)\n",
    "        if size == 2: return self.bigram.setdefault(words, 0)\n",
    "        if size == 3: return self.trigram.setdefault(words, 0)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def clean_text(self, file_name):\n",
    "        result = []\n",
    "        with open(file_name, 'r') as f:\n",
    "            text = f.read()\n",
    "            text = text.lower()\n",
    "            result = []\n",
    "            trantab = str.maketrans(\"?:!-\", \"... \")\n",
    "            text = text.translate(trantab)\n",
    "            trantab = str.maketrans('', '', string.punctuation.replace('.', ''))\n",
    "            text = text.translate(trantab)\n",
    "            text = text.replace('\\n\\n', '.')\n",
    "            tokens = text.split('.')\n",
    "            for token in tokens:\n",
    "                result += ['<s>'] + token.split() + [' </s>']\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def language_m(textFiles):\n",
    "    for i in range(1, 4):\n",
    "        print('\\n\\nNgram:', i)\n",
    "        model = language_model(i)\n",
    "        for file1 in textFiles:\n",
    "            model.train(file1)\n",
    "            print('\\nTrain:', file1)\n",
    "            for file2 in textFiles:\n",
    "                print('Perplexity:', model.test(file2), '\\t(on Test:' + file2 + ')')\n",
    "            #print(model.story)\n",
    "            if i > 1:\n",
    "                outfile = 'model'+str(i)+'.txt'\n",
    "                with open(outfile, encoding='utf-8', mode='w') as fp: \n",
    "                    for tag, count in model.bigram.items():  \n",
    "                        fp.write('{}\\t{}\\n'.format(count, ''.join(tag.replace(' ', '\\t'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, os, json\n",
    "from collections import OrderedDict\n",
    "\n",
    "def getProb(fileName):\n",
    "    corpusPath = \"\"\n",
    "    conditionalProbabilityFile = \"conditionalProbabilityDict.p\"\n",
    "    bigramsListPath = \"bigramsList.p\"\n",
    "\n",
    "    with open(corpusPath+fileName, encoding = \"ISO-8859-1\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    bigramsList = [] # List of all bigrams along with counts. [ [\"24\",\"hello\",\"world\"] , [ ... ], ...]\n",
    "    unigramsDict = OrderedDict() # key : unigram, value : count\n",
    "    singleLine = [] # a temporary variable\n",
    "\n",
    "    for line in lines:\n",
    "        # removing \\n and \\r that were due to readline and splitting by tab\n",
    "        singleLine = line.replace('\\r','').replace('\\n','').split('\\t')\n",
    "        bigramsList.append(singleLine)\n",
    "        # getting all the unigrams W(i-1)\n",
    "        # if key exists then add the count of that unigram\n",
    "        if singleLine[1] in unigramsDict:\n",
    "            unigramsDict[singleLine[1]] += int(singleLine[0])\n",
    "        else:\n",
    "            unigramsDict[singleLine[1]] = int(singleLine[0])\n",
    "\n",
    "    #print(bigramsList)\n",
    "    with open('bigram_json.txt', 'w') as outfile:\n",
    "        json.dump(bigramsList, outfile)\n",
    "    with open('unigramDict_json.txt', 'w') as outfile:\n",
    "        json.dump(unigramsDict, outfile)\n",
    "    #print(unigramsDict)\n",
    "\n",
    "    #all the keys of a unigramsDict are unique unigrams, hence making a list\n",
    "    unigramsList = [] # raw list of all unigrams\n",
    "    for key in unigramsDict:\n",
    "        unigramsList.append(key)\n",
    "\n",
    "    # print unigramsList\n",
    "\n",
    "    # OK so now you have a unigram list as well as bigram list with frequency.\n",
    "    # Now calculating, for each bigram, its conditional probability for a its own unigram\n",
    "    conditionalProbabilityDict = OrderedDict() # key:bigram , value:probability\n",
    "    for bigram in bigramsList:\n",
    "        firstWord = bigram[1]\n",
    "        secondWord = bigram[2]\n",
    "        count = int(bigram[0])\n",
    "        cProb = count*1.0 / unigramsDict[firstWord] if unigramsDict[firstWord] > 0 else 0\n",
    "        conditionalProbabilityDict[firstWord+\" \"+secondWord] = cProb\n",
    "\n",
    "    # print conditionalProbabilityDict\n",
    "    file = open(conditionalProbabilityFile,\"wb\")\n",
    "    pickle.dump(conditionalProbabilityDict,file)\n",
    "\n",
    "    file = open(bigramsListPath,\"wb\")\n",
    "    pickle.dump(bigramsList,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import heapq # for getting top 5\n",
    "\n",
    "def getTopBigram():\n",
    "    conditionalProbabilityFile = \"conditionalProbabilityDict.p\"\n",
    "    bigramsListPath = \"bigramsList.p\"\n",
    "\n",
    "    file = open(conditionalProbabilityFile,\"rb\")\n",
    "    conditionalProbabilityDict = pickle.load(file)\n",
    "\n",
    "    file = open(bigramsListPath,\"rb\")\n",
    "    bigramsList = pickle.load(file)\n",
    "\n",
    "    while True:\n",
    "        checkForThisBigram = input(\"Enter a word to predict its next probable words ('stop' for stopping) : \")\n",
    "\n",
    "        if checkForThisBigram == \"stop\":\n",
    "            break;\n",
    "\n",
    "        # empty the list, for new iteration\n",
    "        matchedBigrams = [] # all bigrams that starts with the inputted word\n",
    "        for bigram in bigramsList:\n",
    "            if checkForThisBigram == bigram[1]:\n",
    "                matchedBigrams.append(bigram[1]+\" \"+bigram[2])\n",
    "\n",
    "        # print matchedBigrams\n",
    "        topDict = {}\n",
    "        for singleBigram in matchedBigrams:\n",
    "            topDict[singleBigram] = conditionalProbabilityDict[singleBigram]\n",
    "\n",
    "        topBigrams = heapq.nlargest(5, topDict, key=topDict.get)\n",
    "        for b in topBigrams:\n",
    "            print(b + \" : \"+str(topDict[b])+\"\\n\")\n",
    "\n",
    "        print(\"\\n\" + \"____________________\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ngram: 1\n",
      "\n",
      "Train: clickbait_data1\n",
      "Perplexity: 584.5619719069464 \t(on Test:clickbait_data1)\n",
      "\n",
      "\n",
      "Ngram: 2\n",
      "\n",
      "Train: clickbait_data1\n",
      "Perplexity: 702.3791240615541 \t(on Test:clickbait_data1)\n",
      "\n",
      "\n",
      "Ngram: 3\n",
      "\n",
      "Train: clickbait_data1\n",
      "Perplexity: 2156.956516117808 \t(on Test:clickbait_data1)\n",
      "Enter a word to predict its next probable words ('stop' for stopping) : how\n",
      "how well : 0.23160762942779292\n",
      "\n",
      "how to : 0.15803814713896458\n",
      "\n",
      "how much : 0.0885558583106267\n",
      "\n",
      "how many : 0.0667574931880109\n",
      "\n",
      "how you : 0.04632152588555858\n",
      "\n",
      "\n",
      "____________________\n",
      "\n",
      "Enter a word to predict its next probable words ('stop' for stopping) : to\n",
      "to be : 0.05602782071097372\n",
      "\n",
      "to make : 0.05564142194744977\n",
      "\n",
      "to the : 0.044435857805255025\n",
      "\n",
      "to know : 0.03207109737248841\n",
      "\n",
      "to get : 0.030525502318392583\n",
      "\n",
      "\n",
      "____________________\n",
      "\n",
      "Enter a word to predict its next probable words ('stop' for stopping) : flirt\n",
      "flirt with : 0.5\n",
      "\n",
      "flirt on : 0.5\n",
      "\n",
      "\n",
      "____________________\n",
      "\n",
      "Enter a word to predict its next probable words ('stop' for stopping) : stop\n"
     ]
    }
   ],
   "source": [
    "clickbait_dataFiles = [\"clickbait_data1\"]\n",
    "non_clickbait_dataFiles = [\"non_clickbait_data1\", \"non_clickbait_data2\", \"non_clickbait_data3\", \"non_clickbait_data4\", \"non_clickbait_data5\"]\n",
    "fileName = [\"model2.txt\", \"model3.txt\"]\n",
    "language_m(clickbait_dataFiles)\n",
    "getProb(fileName[1])\n",
    "getTopBigram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
