{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Clickbait Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team members: Joshua Burris, Caleb Tong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import string\n",
    "from collections import Counter\n",
    "class language_model:\n",
    "    def __init__(self, ngram=1) :\n",
    "        \"\"\"\n",
    "        Initialize a language model\n",
    "        \n",
    "        Parameters:\n",
    "        ngram specifies the type of model:  \n",
    "        unigram (ngram = 1), bigram (ngram = 2) etc.\n",
    "        \"\"\"\n",
    "        self.ngram = ngram\n",
    "        \n",
    "    def train(self, file_name) :\n",
    "        self.story = self.clean_text(file_name)\n",
    "        if self.ngram > 1:\n",
    "            self.bigram = []\n",
    "            for i in range(len(self.story) - 1):\n",
    "                self.bigram.append(self.story[i] + ' ' + self.story[i+1])\n",
    "            self.bigram = Counter(self.bigram)\n",
    "        if self.ngram > 2:\n",
    "            self.trigram = []\n",
    "            for i in range(len(self.story) - self.ngram + 1):\n",
    "                temp = self.story[i]\n",
    "                for j in range(1, self.ngram):\n",
    "                    temp += ' ' + self.story[i+j]\n",
    "                self.trigram.append(temp)\n",
    "            self.trigram = Counter(self.trigram)\n",
    "        self.data_frequency = Counter(self.story)\n",
    "        self.V = len(self.data_frequency)\n",
    "        self.total_count = sum(self.data_frequency.values())\n",
    "        #print(self.total_count, self.V, self.data_frequency)\n",
    "    \n",
    "    def test(self, file_name) :\n",
    "        text = self.clean_text(file_name)\n",
    "        \n",
    "        non_entries, entries = 0, 0\n",
    "        for i in range(len(text) - self.ngram + 1):\n",
    "            temp = text[i]\n",
    "            for j in range(1, self.ngram):\n",
    "                temp += ' ' + text[i+j]\n",
    "            data = {}\n",
    "            if self.ngram == 1:\n",
    "                data = self.data_frequency\n",
    "            elif self.ngram == 2:\n",
    "                data = self.bigram\n",
    "            elif self.ngram == 3:\n",
    "                data = self.trigram\n",
    "            if data.setdefault(temp, 0) == 0:\n",
    "                non_entries += 1\n",
    "            entries += 1\n",
    "        \n",
    "        self.sparsity = non_entries / entries\n",
    "        \n",
    "        return self.perplexity(text)\n",
    "    \n",
    "    def probability(self, word1, words):\n",
    "        if self.ngram == 1:\n",
    "            return (self.C([word1]) + 1) / (self.total_count + self.V)\n",
    "        else:\n",
    "            return (self.C(words + [word1]) + 1) / (self.C(words) + self.V)\n",
    "    \n",
    "    def perplexity(self, text):\n",
    "        return math.pow(2, self.entropy(text))\n",
    "    \n",
    "    def entropy(self, text):\n",
    "        exp = 0\n",
    "        for i in range(self.ngram - 1, len(text)) :\n",
    "            prevW = text[i - self.ngram + 1 : i]\n",
    "            exp += -math.log(self.probability(text[i], prevW), 2)     \n",
    "        return exp / (len(text) - (self.ngram - 1))\n",
    "    \n",
    "    def C(self, words):\n",
    "        size = len(words)\n",
    "        words = ' '.join(words)\n",
    "        if size == 1: return self.data_frequency.setdefault(words, 0)\n",
    "        if size == 2: return self.bigram.setdefault(words, 0)\n",
    "        if size == 3: return self.trigram.setdefault(words, 0)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def clean_text(self, file_name):\n",
    "        result = []\n",
    "        with open(file_name, 'r') as f:\n",
    "            text = f.read()\n",
    "            text = text.lower()\n",
    "            result = []\n",
    "            trantab = str.maketrans(\"?:!-\", \"... \")\n",
    "            text = text.translate(trantab)\n",
    "            trantab = str.maketrans('', '', string.punctuation.replace('.', ''))\n",
    "            text = text.translate(trantab)\n",
    "            text = text.replace('\\n\\n', '.')\n",
    "            tokens = text.split('.')\n",
    "            for token in tokens:\n",
    "                result += ['<s>'] + token.split() + [' </s>']\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def language_m(textFiles):\n",
    "    for i in range(1, 4):\n",
    "        print('\\n\\nNgram:', i)\n",
    "        model = language_model(i)\n",
    "        for file1 in textFiles:\n",
    "            model.train(file1)\n",
    "            print('\\nTrain:', file1)\n",
    "            for file2 in textFiles:\n",
    "                print('Perplexity:', model.test(file2), '\\t(on Test:' + file2 + ')')\n",
    "            #print(model.story)\n",
    "            if i > 1:\n",
    "                outfile = 'model'+str(i)+'.txt'\n",
    "                with open(outfile, encoding='utf-8', mode='w') as fp: \n",
    "                    for tag, count in model.bigram.items():  \n",
    "                        fp.write('{}\\t{}\\n'.format(count, ''.join(tag.replace(' ', '\\t'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, os, json\n",
    "from collections import OrderedDict\n",
    "\n",
    "def getProb(fileName):\n",
    "    corpusPath = \"\"\n",
    "    conditionalProbabilityFile = \"conditionalProbabilityDict.p\"\n",
    "    bigramsListPath = \"bigramsList.p\"\n",
    "    unigramProbFile = \"unigramProbDict.p\"\n",
    "    unigramsDictPath = \"unigramsDict.p\"\n",
    "\n",
    "    with open(corpusPath+fileName, encoding = \"ISO-8859-1\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    bigramsList = [] # List of all bigrams along with counts. [ [\"24\",\"hello\",\"world\"] , [ ... ], ...]\n",
    "    unigramsDict = OrderedDict() # key : unigram, value : count\n",
    "    singleLine = [] # a temporary variable\n",
    "\n",
    "    for line in lines:\n",
    "        # removing \\n and \\r that were due to readline and splitting by tab\n",
    "        singleLine = line.replace('\\r','').replace('\\n','').split('\\t')\n",
    "        if '' in singleLine:\n",
    "            singleLine.remove('')\n",
    "        bigramsList.append(singleLine)\n",
    "        # getting all the unigrams W(i-1)\n",
    "        # if key exists then add the count of that unigram\n",
    "        if singleLine[1] in unigramsDict:\n",
    "            unigramsDict[singleLine[1]] += int(singleLine[0])\n",
    "        else:\n",
    "            unigramsDict[singleLine[1]] = int(singleLine[0])\n",
    "\n",
    "    #print(bigramsList[:10]) #[['31', '<s>', 'should'], ['2', 'should', 'i'], ['1', 'i', 'get'], \n",
    "                             # ['1', 'get', 'bings'], ['1', 'bings', '', '</s>'], ['13491', '', '</s>', '<s>'], \n",
    "                             # ['592', '<s>', 'which'], ['17', 'which', 'tv'], ['1', 'tv', 'female'], \n",
    "                             # ['1', 'female', 'friend'], ...]\n",
    "                \n",
    "    #print(unigramsDict) # OrderedDict([('<s>', 13492), ('should', 498), ('i', 229), ('get', 258), ('bings', 1), \n",
    "                         #              ('', 13491), ('which', 700), ('tv', 95), ('female', 25), ... ])\n",
    "    with open('bigram_json.txt', 'w') as outfile:\n",
    "        json.dump(bigramsList, outfile)\n",
    "    with open('unigramDict_json.txt', 'w') as outfile:\n",
    "        json.dump(unigramsDict, outfile)\n",
    "\n",
    "    #all the keys of a unigramsDict are unique unigrams, hence making a list\n",
    "    unigramsList = [] # raw list of all unigrams\n",
    "    for key in unigramsDict:\n",
    "        unigramsList.append(key)\n",
    "    #print(unigramsList[:10]) #['<s>', 'should', 'i', 'get', 'bings', '', 'which', 'tv', 'female', 'friend', ...]\n",
    "\n",
    "    # OK so now you have a unigram list as well as bigram list with frequency.\n",
    "    # Now calculating, for each bigram, its conditional probability for a its own unigram\n",
    "    conditionalProbabilityDict = OrderedDict() # key:bigram , value:probability\n",
    "    for bigram in bigramsList:\n",
    "        firstWord = bigram[1]\n",
    "        secondWord = bigram[2]\n",
    "        count = int(bigram[0])\n",
    "        cProb = count*1.0 / unigramsDict[firstWord] if unigramsDict[firstWord] > 0 else 0\n",
    "        conditionalProbabilityDict[firstWord+\" \"+secondWord] = cProb\n",
    "    \n",
    "    unigramProbDict = OrderedDict()\n",
    "    unigram_total = sum(unigramsDict.values())\n",
    "    for unigram, count in unigramsDict.items():\n",
    "        uProb = count/unigram_total\n",
    "        unigramProbDict[unigram] = uProb\n",
    "        #print(unigram, count, unigram_total, uProb)\n",
    "\n",
    "    # print conditionalProbabilityDict\n",
    "    with open(conditionalProbabilityFile,\"wb\") as file:\n",
    "        pickle.dump(conditionalProbabilityDict,file)\n",
    "\n",
    "    with open(bigramsListPath,\"wb\") as file:\n",
    "        pickle.dump(bigramsList,file)\n",
    "    \n",
    "    with open(unigramProbFile, \"wb\") as file:\n",
    "        pickle.dump(unigramProbDict, file)\n",
    "        \n",
    "    with open(unigramsDictPath, \"wb\") as file:\n",
    "        pickle.dump(unigramsDict, file)\n",
    "    \n",
    "    #print(conditionalProbabilityDict) # OrderedDict([('<s> should', 0.0022976578713311594), \n",
    "                                       #              ('should i', 0.004016064257028112), \n",
    "                                       #              ('i get', 0.004366812227074236), \n",
    "                                       #              ('get bings', 0.003875968992248062), ('bings ', 1.0), ... ])\n",
    "    \n",
    "    #print(bigramsList) #[['31', '<s>', 'should'], ['2', 'should', 'i'], ['1', 'i', 'get'], \n",
    "                        # ['1', 'get', 'bings'], ['1', 'bings', '', '</s>'], ['13491', '', '</s>', '<s>'], \n",
    "                        # ['592', '<s>', 'which'], ['17', 'which', 'tv'], ['1', 'tv', 'female'], \n",
    "                        # ['1', 'female', 'friend'], ...]\n",
    "                \n",
    "    #print(unigramProbDict) #OrderedDict([('<s>', 0.08680323228164084), ('should', 0.003203973441762314), \n",
    "                            #             ('i', 0.0014733130886818674), ('get', 0.0016598898553708374), \n",
    "                            #             ('bings', 6.433681609964486e-06), ... ]) \n",
    "    \n",
    "    #print(unigramsDict) # OrderedDict([('<s>', 13492), ('should', 498), ('i', 229), ('get', 258), ('bings', 1), \n",
    "                         #              ('', 13491), ('which', 700), ('tv', 95), ('female', 25), ... ])\\\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import heapq # for getting top 5\n",
    "\n",
    "def getTopBigram():\n",
    "    conditionalProbabilityFile = \"conditionalProbabilityDict.p\"\n",
    "    bigramsListPath = \"bigramsList.p\"\n",
    "    unigramProbFile = \"unigramProbDict.p\"\n",
    "    unigramsDictPath = \"unigramsDict.p\"\n",
    "\n",
    "    with open(conditionalProbabilityFile,\"rb\") as file:\n",
    "        conditionalProbabilityDict = pickle.load(file)\n",
    "\n",
    "    with open(bigramsListPath,\"rb\") as file:\n",
    "        bigramsList = pickle.load(file)\n",
    "    \n",
    "    with open(unigramProbFile,\"rb\") as file:\n",
    "        unigramProbDict = pickle.load(file)\n",
    "\n",
    "    with open(unigramsDictPath,\"rb\") as file:\n",
    "        unigramsDict = pickle.load(file)\n",
    "    \n",
    "    while True:\n",
    "        sentence = input(\"Enter a sentence to predict whether it is clickbait or not ('stop' for stopping) : \\n\")\n",
    "\n",
    "        if sentence == \"stop\":\n",
    "            break;\n",
    "        newSentence = [\"<s>\"] + sentence.lower().split() + [\"</s>\"]\n",
    "        lengthOfSentence = len(newSentence) - 2\n",
    "        \n",
    "        wordsOf2 = []\n",
    "        totalProb = unigramProbDict[\"<s>\"]\n",
    "        for index, word in enumerate(newSentence):\n",
    "            if word != \"</s>\":\n",
    "                wordsOf2.append(word + \" \" + newSentence[index+1])\n",
    "        end = 0\n",
    "        for bigram in wordsOf2:\n",
    "            if bigram in conditionalProbabilityDict:\n",
    "                totalProb *= conditionalProbabilityDict[bigram]\n",
    "            else:\n",
    "                print(bigram)\n",
    "                print(\"TODO: needs smoothing\")\n",
    "                end = 1\n",
    "        if end == 1:\n",
    "            break\n",
    "        print(\"\\n\\nProbability of:\\n\" + \"\\t\" + sentence + \"\\nbeing clickbait is:\", totalProb)\n",
    "\n",
    "        # empty the list, for new iteration\n",
    "        #matchedBigrams = [] # all bigrams that starts with the inputted word\n",
    "        #for bigram in bigramsList:\n",
    "        #    if sentence == bigram[1]:\n",
    "        #        matchedBigrams.append(bigram[1]+\" \"+bigram[2])\n",
    "\n",
    "        # print matchedBigrams\n",
    "        #topDict = {}\n",
    "        #for singleBigram in matchedBigrams:\n",
    "        #    print(singleBigram, type(singleBigram), conditionalProbabilityDict[singleBigram], type(conditionalProbabilityDict[singleBigram]))\n",
    "        #    topDict[singleBigram] = conditionalProbabilityDict[singleBigram]\n",
    "\n",
    "        #topBigrams = heapq.nlargest(5, topDict, key=topDict.get)\n",
    "        #for b in topBigrams:\n",
    "        #    print(b + \" : \"+str(topDict[b])+\"\\n\")\n",
    "\n",
    "        print(\"\\n\" + \"____________________\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ngram: 1\n",
      "\n",
      "Train: train/clickbait_data1\n",
      "Perplexity: 584.5619719069464 \t(on Test:train/clickbait_data1)\n",
      "\n",
      "\n",
      "Ngram: 2\n",
      "\n",
      "Train: train/clickbait_data1\n",
      "Perplexity: 702.3791240615541 \t(on Test:train/clickbait_data1)\n",
      "\n",
      "\n",
      "Ngram: 3\n",
      "\n",
      "Train: train/clickbait_data1\n",
      "Perplexity: 2156.956516117808 \t(on Test:train/clickbait_data1)\n",
      "Enter a sentence to predict whether it is clickbait or not ('stop' for stopping) : \n",
      "If Disney Princesses Were From Florida\n",
      "\n",
      "\n",
      "Probability of:\n",
      "\tIf Disney Princesses Were From Florida\n",
      "being clickbait is: 4.5151277394698045e-13\n",
      "\n",
      "____________________\n",
      "\n",
      "Enter a sentence to predict whether it is clickbait or not ('stop' for stopping) : \n",
      "Who Is Your Celebrity Ex Based On Your Zodiac\n",
      "\n",
      "\n",
      "Probability of:\n",
      "\tWho Is Your Celebrity Ex Based On Your Zodiac\n",
      "being clickbait is: 1.022632013180572e-13\n",
      "\n",
      "____________________\n",
      "\n",
      "Enter a sentence to predict whether it is clickbait or not ('stop' for stopping) : \n",
      "How To Be A Genderqueer Feminist\n",
      "a genderqueer\n",
      "TODO: needs smoothing\n",
      "genderqueer feminist\n",
      "TODO: needs smoothing\n"
     ]
    }
   ],
   "source": [
    "clickbait_dataFiles = [\"train/clickbait_data1\"]\n",
    "non_clickbait_dataFiles = [\"non_clickbait_data1\", \"non_clickbait_data2\", \"non_clickbait_data3\", \"non_clickbait_data4\", \"non_clickbait_data5\"]\n",
    "fileName = [\"model1.txt\", \"model3.txt\"]\n",
    "language_m(clickbait_dataFiles)\n",
    "getProb(fileName[-1])\n",
    "getTopBigram()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
