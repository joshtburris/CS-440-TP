{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Clickbait Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team members: Joshua Burris, Caleb Tong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import string\n",
    "from collections import Counter\n",
    "class language_model:\n",
    "    def __init__(self, ngram=1) :\n",
    "        \"\"\"\n",
    "        Initialize a language model\n",
    "        \n",
    "        Parameters:\n",
    "        ngram specifies the type of model:  \n",
    "        unigram (ngram = 1), bigram (ngram = 2) etc.\n",
    "        \"\"\"\n",
    "        self.ngram = ngram\n",
    "        \n",
    "    def train(self, file_name) :\n",
    "        self.story = self.clean_text(file_name)\n",
    "        if self.ngram > 1:\n",
    "            self.bigram = []\n",
    "            for i in range(len(self.story) - 1):\n",
    "                self.bigram.append(self.story[i] + ' ' + self.story[i+1])\n",
    "            self.bigram = Counter(self.bigram)\n",
    "        if self.ngram > 2:\n",
    "            self.trigram = []\n",
    "            for i in range(len(self.story) - self.ngram + 1):\n",
    "                temp = self.story[i]\n",
    "                for j in range(1, self.ngram):\n",
    "                    temp += ' ' + self.story[i+j]\n",
    "                self.trigram.append(temp)\n",
    "            self.trigram = Counter(self.trigram)\n",
    "        self.data_frequency = Counter(self.story)\n",
    "        self.V = len(self.data_frequency)\n",
    "        self.total_count = sum(self.data_frequency.values())\n",
    "        #print(self.total_count, self.V, self.data_frequency)\n",
    "    \n",
    "    def test(self, file_name) :\n",
    "        text = self.clean_text(file_name)\n",
    "        \n",
    "        non_entries, entries = 0, 0\n",
    "        for i in range(len(text) - self.ngram + 1):\n",
    "            temp = text[i]\n",
    "            for j in range(1, self.ngram):\n",
    "                temp += ' ' + text[i+j]\n",
    "            data = {}\n",
    "            if self.ngram == 1:\n",
    "                data = self.data_frequency\n",
    "            elif self.ngram == 2:\n",
    "                data = self.bigram\n",
    "            elif self.ngram == 3:\n",
    "                data = self.trigram\n",
    "            if data.setdefault(temp, 0) == 0:\n",
    "                non_entries += 1\n",
    "            entries += 1\n",
    "        \n",
    "        self.sparsity = non_entries / entries\n",
    "        \n",
    "        return self.perplexity(text)\n",
    "    \n",
    "    def probability(self, word1, words):\n",
    "        if self.ngram == 1:\n",
    "            return (self.C([word1]) + 1) / (self.total_count + self.V)\n",
    "        else:\n",
    "            return (self.C(words + [word1]) + 1) / (self.C(words) + self.V)\n",
    "    \n",
    "    def perplexity(self, text):\n",
    "        return math.pow(2, self.entropy(text))\n",
    "    \n",
    "    def entropy(self, text):\n",
    "        exp = 0\n",
    "        for i in range(self.ngram - 1, len(text)) :\n",
    "            prevW = text[i - self.ngram + 1 : i]\n",
    "            exp += -math.log(self.probability(text[i], prevW), 2)     \n",
    "        return exp / (len(text) - (self.ngram - 1))\n",
    "    \n",
    "    def C(self, words):\n",
    "        size = len(words)\n",
    "        words = ' '.join(words)\n",
    "        if size == 1: return self.data_frequency.setdefault(words, 0)\n",
    "        if size == 2: return self.bigram.setdefault(words, 0)\n",
    "        if size == 3: return self.trigram.setdefault(words, 0)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def clean_text(self, file_name):\n",
    "        result = []\n",
    "        with open(file_name, 'r') as f:\n",
    "            text = f.read()\n",
    "            text = text.lower()\n",
    "            result = []\n",
    "            trantab = str.maketrans(\"?:!-\", \"... \")\n",
    "            text = text.translate(trantab)\n",
    "            trantab = str.maketrans('', '', string.punctuation.replace('.', ''))\n",
    "            text = text.translate(trantab)\n",
    "            text = text.replace('\\n\\n', '.')\n",
    "            tokens = text.split('.')\n",
    "            for token in tokens:\n",
    "                result += ['<s>'] + token.split() + [' </s>']\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ngram: 1\n",
      "\n",
      "Train: clickbait_data\n",
      "Perplexity: 586.2356890939692 \t(on Test:clickbait_data)\n",
      "Perplexity: 4252.524250034634 \t(on Test:non_clickbait_data)\n",
      "\n",
      "Train: non_clickbait_data\n",
      "Perplexity: 2507.3471467753056 \t(on Test:clickbait_data)\n",
      "Perplexity: 1331.195305394055 \t(on Test:non_clickbait_data)\n",
      "\n",
      "\n",
      "Ngram: 2\n",
      "\n",
      "Train: clickbait_data\n",
      "Perplexity: 686.9818177941813 \t(on Test:clickbait_data)\n",
      "Perplexity: 4583.854947135656 \t(on Test:non_clickbait_data)\n",
      "\n",
      "Train: non_clickbait_data\n",
      "Perplexity: 5653.572136562472 \t(on Test:clickbait_data)\n",
      "Perplexity: 2350.5540437435843 \t(on Test:non_clickbait_data)\n",
      "\n",
      "\n",
      "Ngram: 3\n",
      "\n",
      "Train: clickbait_data\n",
      "Perplexity: 2200.396748749142 \t(on Test:clickbait_data)\n",
      "Perplexity: 10667.660035826613 \t(on Test:non_clickbait_data)\n",
      "\n",
      "Train: non_clickbait_data\n",
      "Perplexity: 14824.160725682368 \t(on Test:clickbait_data)\n",
      "Perplexity: 5870.082927134902 \t(on Test:non_clickbait_data)\n"
     ]
    }
   ],
   "source": [
    "textFiles = [\"clickbait_data\", \"non_clickbait_data\"]\n",
    "for i in range(1,4):\n",
    "    print('\\n\\nNgram:', i)\n",
    "    model = language_model(i)\n",
    "    for file1 in textFiles:\n",
    "        model.train(file1)\n",
    "        print('\\nTrain:', file1)\n",
    "        for file2 in textFiles:\n",
    "            print('Perplexity:', model.test(file2), '\\t(on Test:' + file2 + ')')\n",
    "        #print(model.story)\n",
    "        if i > 1:\n",
    "            outfile = 'model'+str(i)+'.txt'\n",
    "            with open(outfile, encoding='utf-8', mode='w') as fp: \n",
    "                for tag, count in model.bigram.items():  \n",
    "                    fp.write('{}\\t{}\\n'.format(count, ''.join(tag.replace(' ', '\\t'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, os, json\n",
    "from collections import OrderedDict\n",
    "\n",
    "corpusPath = \"\"\n",
    "fileName = \"model2.txt\"\n",
    "conditionalProbabilityFile = \"conditionalProbabilityDict.p\"\n",
    "bigramsListPath = \"bigramsList.p\"\n",
    "\n",
    "with open(corpusPath+fileName, encoding = \"ISO-8859-1\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "bigramsList = [] # List of all bigrams along with counts. [ [\"24\",\"hello\",\"world\"] , [ ... ], ...]\n",
    "unigramsDict = OrderedDict() # key : unigram, value : count\n",
    "singleLine = [] # a temporary variable\n",
    "\n",
    "for line in lines:\n",
    "    # removing \\n and \\r that were due to readline and splitting by tab\n",
    "    singleLine = line.replace('\\r','').replace('\\n','').split('\\t')\n",
    "    bigramsList.append(singleLine)\n",
    "    # getting all the unigrams W(i-1)\n",
    "    # if key exists then add the count of that unigram\n",
    "    if singleLine[1] in unigramsDict:\n",
    "        unigramsDict[singleLine[1]] += int(singleLine[0])\n",
    "    else:\n",
    "        unigramsDict[singleLine[1]] = int(singleLine[0])\n",
    "\n",
    "#print(bigramsList)\n",
    "with open('bigram_json.txt', 'w') as outfile:\n",
    "    json.dump(bigramsList, outfile)\n",
    "with open('unigramDict_json.txt', 'w') as outfile:\n",
    "    json.dump(unigramsDict, outfile)\n",
    "#print(unigramsDict)\n",
    "\n",
    "#all the keys of a unigramsDict are unique unigrams, hence making a list\n",
    "unigramsList = [] # raw list of all unigrams\n",
    "for key in unigramsDict:\n",
    "    unigramsList.append(key)\n",
    "\n",
    "# print unigramsList\n",
    "\n",
    "# OK so now you have a unigram list as well as bigram list with frequency.\n",
    "# Now calculating, for each bigram, its conditional probability for a its own unigram\n",
    "conditionalProbabilityDict = OrderedDict() # key:bigram , value:probability\n",
    "for bigram in bigramsList:\n",
    "    firstWord = bigram[1]\n",
    "    secondWord = bigram[2]\n",
    "    count = int(bigram[0])\n",
    "    cProb = count*1.0 / unigramsDict[firstWord] if unigramsDict[firstWord] > 0 else 0\n",
    "    conditionalProbabilityDict[firstWord+\" \"+secondWord] = cProb\n",
    "\n",
    "# print conditionalProbabilityDict\n",
    "file = open(conditionalProbabilityFile,\"wb\")\n",
    "pickle.dump(conditionalProbabilityDict,file)\n",
    "\n",
    "file = open(bigramsListPath,\"wb\")\n",
    "pickle.dump(bigramsList,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a word to predict its next probable words ('stop' for stopping) : stop\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import heapq # for getting top 5\n",
    "\n",
    "conditionalProbabilityFile = \"conditionalProbabilityDict.p\"\n",
    "bigramsListPath = \"bigramsList.p\"\n",
    "\n",
    "file = open(conditionalProbabilityFile,\"rb\")\n",
    "conditionalProbabilityDict = pickle.load(file)\n",
    "\n",
    "file = open(bigramsListPath,\"rb\")\n",
    "bigramsList = pickle.load(file)\n",
    "\n",
    "while True:\n",
    "    checkForThisBigram = input(\"Enter a word to predict its next probable words ('stop' for stopping) : \")\n",
    "\n",
    "    if checkForThisBigram == \"stop\":\n",
    "        break;\n",
    "\n",
    "    # empty the list, for new iteration\n",
    "    matchedBigrams = [] # all bigrams that starts with the inputted word\n",
    "    for bigram in bigramsList:\n",
    "        if checkForThisBigram == bigram[1]:\n",
    "            matchedBigrams.append(bigram[1]+\" \"+bigram[2])\n",
    "\n",
    "    # print matchedBigrams\n",
    "    topDict = {}\n",
    "    for singleBigram in matchedBigrams:\n",
    "        topDict[singleBigram] = conditionalProbabilityDict[singleBigram]\n",
    "\n",
    "    topBigrams = heapq.nlargest(5, topDict, key=topDict.get)\n",
    "    for b in topBigrams:\n",
    "        print(b + \" : \"+str(topDict[b])+\"\\n\")\n",
    "\n",
    "    print(\"\\n\" + \"____________________\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
